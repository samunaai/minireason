/home/kentaro.inui/sam/minireason/run_countdown_experiment.py:675: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=False)
[RANK 0] Loading data from cache: countdown_data_100000_samples_6_digit_tokenized.pt

==============================================================
                      Run Configuration                       
==============================================================
                  --- System & Hardware ---                   
PyTorch Version:             2.7.1+cu126
Available GPUs:              4

                     --- Data & Vocab ---                     
Total Dataset Size:          100,000 samples
Train / Validation Split:    90,000 / 10,000
Problem Type:                6-number Countdown
Vocabulary Size:             25 tokens

                  --- Model Architecture ---                  
d_model / n_heads / n_layers: 128 / 4 / 4
Total Parameters:            824,064 
Max Sequence Length:         215 (prompt=21, solution=193)
Seq Len Cap (env):           2048

               --- Training Hyperparameters ---               
Epochs (Max):                1000
Early Stopping Patience:     20
Batch Size:                  32
Learning Rate:               0.0003
Weight Decay:                0.01
Gradient Checkpointing:      Disabled
Random Seed:                 42
==============================================================

Starting training with a single GPU...
/home/kentaro.inui/sam/minireason/run_countdown_experiment.py:469: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_amp):
/home/kentaro.inui/sam/minireason/run_countdown_experiment.py:497: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_amp):
Epoch 0001 | Train loss 1.3794 | Val loss 0.4652
  -> New best val loss. Checkpoint saved.
Epoch 0002 | Train loss 0.3160 | Val loss 0.2026
  -> New best val loss. Checkpoint saved.
Epoch 0003 | Train loss 0.1670 | Val loss 0.1418
  -> New best val loss. Checkpoint saved.
Epoch 0004 | Train loss 0.1289 | Val loss 0.1201
  -> New best val loss. Checkpoint saved.
Epoch 0005 | Train loss 0.1096 | Val loss 0.1082
  -> New best val loss. Checkpoint saved.
Epoch 0006 | Train loss 0.0980 | Val loss 0.0959
  -> New best val loss. Checkpoint saved.
Epoch 0007 | Train loss 0.0902 | Val loss 0.0892
  -> New best val loss. Checkpoint saved.
Epoch 0008 | Train loss 0.0844 | Val loss 0.0861
  -> New best val loss. Checkpoint saved.
Epoch 0009 | Train loss 0.0799 | Val loss 0.0829
  -> New best val loss. Checkpoint saved.
Epoch 0010 | Train loss 0.0765 | Val loss 0.0782
  -> New best val loss. Checkpoint saved.
Epoch 0011 | Train loss 0.0737 | Val loss 0.0772
  -> New best val loss. Checkpoint saved.
Epoch 0012 | Train loss 0.0712 | Val loss 0.0746
  -> New best val loss. Checkpoint saved.
Epoch 0013 | Train loss 0.0692 | Val loss 0.0725
  -> New best val loss. Checkpoint saved.
Epoch 0014 | Train loss 0.0676 | Val loss 0.0714
  -> New best val loss. Checkpoint saved.
Epoch 0015 | Train loss 0.0659 | Val loss 0.0700
  -> New best val loss. Checkpoint saved.
Epoch 0016 | Train loss 0.0646 | Val loss 0.0688
  -> New best val loss. Checkpoint saved.
Epoch 0017 | Train loss 0.0636 | Val loss 0.0688
  -> New best val loss. Checkpoint saved.
Epoch 0018 | Train loss 0.0624 | Val loss 0.0664
  -> New best val loss. Checkpoint saved.
Epoch 0019 | Train loss 0.0614 | Val loss 0.0669
Epoch 0020 | Train loss 0.0606 | Val loss 0.0657
  -> New best val loss. Checkpoint saved.
Epoch 0021 | Train loss 0.0597 | Val loss 0.0655
  -> New best val loss. Checkpoint saved.
Epoch 0022 | Train loss 0.0590 | Val loss 0.0642
  -> New best val loss. Checkpoint saved.
Epoch 0023 | Train loss 0.0582 | Val loss 0.0639
  -> New best val loss. Checkpoint saved.
Epoch 0024 | Train loss 0.0576 | Val loss 0.0629
  -> New best val loss. Checkpoint saved.
Epoch 0025 | Train loss 0.0570 | Val loss 0.0631
Epoch 0026 | Train loss 0.0564 | Val loss 0.0652
Epoch 0027 | Train loss 0.0557 | Val loss 0.0629
  -> New best val loss. Checkpoint saved.
Epoch 0028 | Train loss 0.0553 | Val loss 0.0627
  -> New best val loss. Checkpoint saved.
Epoch 0029 | Train loss 0.0547 | Val loss 0.0615
  -> New best val loss. Checkpoint saved.
Epoch 0030 | Train loss 0.0543 | Val loss 0.0618
Epoch 0031 | Train loss 0.0537 | Val loss 0.0614
  -> New best val loss. Checkpoint saved.
Epoch 0032 | Train loss 0.0533 | Val loss 0.0622
Epoch 0033 | Train loss 0.0528 | Val loss 0.0613
  -> New best val loss. Checkpoint saved.
Epoch 0034 | Train loss 0.0524 | Val loss 0.0602
  -> New best val loss. Checkpoint saved.
Epoch 0035 | Train loss 0.0521 | Val loss 0.0609
Epoch 0036 | Train loss 0.0516 | Val loss 0.0609
Epoch 0037 | Train loss 0.0512 | Val loss 0.0600
  -> New best val loss. Checkpoint saved.
Epoch 0038 | Train loss 0.0509 | Val loss 0.0610
Epoch 0039 | Train loss 0.0505 | Val loss 0.0605
Epoch 0040 | Train loss 0.0501 | Val loss 0.0607
Epoch 0041 | Train loss 0.0498 | Val loss 0.0611
Epoch 0042 | Train loss 0.0494 | Val loss 0.0606
Epoch 0043 | Train loss 0.0491 | Val loss 0.0608
Epoch 0044 | Train loss 0.0488 | Val loss 0.0608
Epoch 0045 | Train loss 0.0484 | Val loss 0.0603
Epoch 0046 | Train loss 0.0482 | Val loss 0.0615
Epoch 0047 | Train loss 0.0478 | Val loss 0.0609
Epoch 0048 | Train loss 0.0475 | Val loss 0.0603
Epoch 0049 | Train loss 0.0472 | Val loss 0.0612
Epoch 0050 | Train loss 0.0469 | Val loss 0.0611
Epoch 0051 | Train loss 0.0466 | Val loss 0.0612
Epoch 0052 | Train loss 0.0463 | Val loss 0.0617
Epoch 0053 | Train loss 0.0461 | Val loss 0.0610
Epoch 0054 | Train loss 0.0457 | Val loss 0.0611
Epoch 0055 | Train loss 0.0455 | Val loss 0.0620
Epoch 0056 | Train loss 0.0452 | Val loss 0.0619
Epoch 0057 | Train loss 0.0450 | Val loss 0.0632
Stopping early.

Training plot saved to countdown_training_plot.png

Loading best model for final evaluation...
/home/kentaro.inui/sam/minireason/run_countdown_experiment.py:523: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_amp):

--- Final Evaluation Metrics ---
  Program Exact Match:  0.00%
  Verified Answer Acc:  0.00%
----------------------------------
on 128 validation samples.

--- Inference Examples ---

--- Example #1 ---
Problem:     IN: [ 100975744 ] TGT: 405
True Sol:    [ 447975100 ] -> 4 + 7 = 11 -> [ 491175100 ] ; [ 491175100 ] -> 9 + 11 = 20 -> [ 42075100 ] ; [ 42075100 ] -> 20 + 100 = 120 -> [ 475120 ] ; [ 475120 ] -> 4 * 120 = 480 -> [ 75480 ] ; [ 75480 ] -> 480 - 75 = 405 -> [ 405 ] ; ANSWER
Generated:   [ 447975100 ] -> 4 + 4 = 8 -> [ 78975100 ] ; [ 78975100 ] -> 7 + 8 = 15 -> [ 91575100 ] ; [ 91575100 ] -> 15 + 100 = 115 -> [ 975115 ] ; [ 975115 ] -> 115 - 75 = 30 -> [ 930 ] ; [ 930 ] -> 9 * 30 = 405 -> [ 405 ] ; ANSWER

--- Example #2 ---
Problem:     IN: [ 509255752 ] TGT: 463
True Sol:    [ 259255075 ] -> 2 + 5 = 7 -> [ 79255075 ] ; [ 79255075 ] -> 7 + 50 = 57 -> [ 9255775 ] ; [ 9255775 ] -> 9 * 57 = 513 -> [ 2575513 ] ; [ 2575513 ] -> 25 + 513 = 538 -> [ 75538 ] ; [ 75538 ] -> 538 - 75 = 463 -> [ 463 ] ; ANSWER
Generated:   [ 259255075 ] -> 2 + 5 = 7 -> [ 79255075 ] ; [ 79255075 ] -> 7 + 50 = 57 -> [ 9255775 ] ; [ 9255775 ] -> 75 - 57 = 18 -> [ 91825 ] ; [ 91825 ] -> 18 * 25 = 450 -> [ 9450 ] ; [ 9450 ] -> 9 + 450 = 463 -> [ 463 ] ; ANSWER

--- Example #3 ---
Problem:     IN: [ 51005012575 ] TGT: 189
True Sol:    [ 15255075100 ] -> 1 + 50 = 51 -> [ 5255175100 ] ; [ 5255175100 ] -> 5 * 75 = 375 -> [ 2551100375 ] ; [ 2551100375 ] -> 51 * 100 = 5100 -> [ 253755100 ] ; [ 253755100 ] -> 5100 - 375 = 4725 -> [ 254725 ] ; [ 254725 ] -> 4725 / 25 = 189 -> [ 189 ] ; ANSWER
Generated:   [ 15255075100 ] -> 1 + 5 = 6 -> [ 6255075100 ] ; [ 6255075100 ] -> 6 + 50 = 56 -> [ 255675100 ] ; [ 255675100 ] -> 56 * 75 = 4200 -> [ 251004200 ] ; [ 251004200 ] -> 4200 / 25 = 168 -> [ 100168 ] ; [ 100168 ] -> 100 + 168 = 189 -> [ 189 ] ; ANSWER

--- Example #4 ---
Problem:     IN: [ 4258217 ] TGT: 483
True Sol:    [ 1247825 ] -> 1 + 2 = 3 -> [ 347825 ] ; [ 347825 ] -> 3 * 4 = 12 -> [ 781225 ] ; [ 781225 ] -> 7 + 12 = 19 -> [ 81925 ] ; [ 81925 ] -> 19 * 25 = 475 -> [ 8475 ] ; [ 8475 ] -> 8 + 475 = 483 -> [ 483 ] ; ANSWER
Generated:   [ 1247825 ] -> 1 + 2 = 3 -> [ 347825 ] ; [ 347825 ] -> 3 * 7 = 21 -> [ 482125 ] ; [ 482125 ] -> 4 + 8 = 12 -> [ 122125 ] ; [ 122125 ] -> 12 + 25 = 37 -> [ 2137 ] ; [ 2137 ] -> 21 * 37 = 483 -> [ 483 ] ; ANSWER

--- Example #5 ---
Problem:     IN: [ 32517510050 ] TGT: 194
True Sol:    [ 13255075100 ] -> 1 + 3 = 4 -> [ 4255075100 ] ; [ 4255075100 ] -> 25 * 100 = 2500 -> [ 450752500 ] ; [ 450752500 ] -> 2500 - 75 = 2425 -> [ 4502425 ] ; [ 4502425 ] -> 4 * 2425 = 9700 -> [ 509700 ] ; [ 509700 ] -> 9700 / 50 = 194 -> [ 194 ] ; ANSWER
Generated:   [ 13255075100 ] -> 1 + 3 = 4 -> [ 4255075100 ] ; [ 4255075100 ] -> 4 + 25 = 29 -> [ 295075100 ] ; [ 295075100 ] -> 29 + 75 = 104 -> [ 50100104 ] ; [ 50100104 ] -> 100 + 104 = 204 -> [ 50204 ] ; [ 50204 ] -> 204 - 50 = 194 -> [ 194 ] ; ANSWER
